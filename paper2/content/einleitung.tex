\section{Einleitung}

\subsection{Motivation}
Computer werden immer leistungsfähiger.
Bis zur Mitte des ersten Jahrzehnts nach der Jahrtausendwende stieg der
Prozessortakt kontinuierlich an und die Ausführung einzelner Instruktionen
wurde optimiert.
Allerdings stagniert der Trend seitdem.
Es sind physikalische Limits erreicht; ein weiterer Anstieg kann nur noch
mit hohem Energieverbrauch und viel Abwärme erkauft werden.
Der Nutzen wird einschränkt.
Die Aussagen bedeuten jedoch nicht, dass moderne Prozessoren nicht effizienter
sein können.
Die Multicore-Architekturen setzen mehrere Prozessoren auf einem Chip ein,
zudem wird Hyper-Threading standardmäßig genutzt.
Aber ein Programm läuft nicht von allein viermal so schnell, wenn man es auf
einer Maschine mit Dualcore Prozessor einsetzt.
Im Gegensatz zur Erhöhung der Taktfrequenz, kann Software nicht einfach mehrere
Kerne verwenden.
Wie Herb Sutter in seinem Artikel „The Free Lunch Is Over“
\cite{sutterlunch} beschreibt, ist es heute notwendig, die Programme anzupassen
und es auszunutzen, dass nebenläufige Ausführung möglich ist.
Man kann nicht mehr darauf vertrauen, dass die Software mit jeder Generation
von Prozessoren schneller läuft.
Die Algorithmen müssen angepasst und parallelisiert werden, falls möglich, um
eine effiziente Auslastung von modernen Computer zu gewährleisten.

Diese Ausarbeitung soll einen groben Überblick über parallele Algorithmen geben.
Es werden theoretische Modelle vorgestellt, anhand derer Algorithmen analysiert
werden können, und es wird auf die Frage eingegangen, welche Probleme überhaupt
parallel gelöst werden können.

\subsection{Begriffe}
Zum Verständnis werden im Folgenden einige Begriffe eingeführt.

\paragraph{Zeitkomplexität}
Die Ausführungszeit eines Algorithmus wird mit der Anzahl der ausgeführten Instruktionen angegeben.

\paragraph{Platzkomplexität}
Der Speicherbedarf ist die Menge an Variablen, in denen Werte während der Ausführung zwischengespeichert werden.

\paragraph{Operationenkomplexität}
Die Summe aller ausgeführten Operationen eines Algorithmus. Bei sequentiellen Algorithmen entspricht diese der Zeitkomplexität.
