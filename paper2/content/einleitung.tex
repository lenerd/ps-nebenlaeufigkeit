\section{Einleitung}

\subsection{Motivation}
Computer werden immer leistungsfähiger.
Bis zur Mitte des ersten Jahrzehnts nach der Jahrtausendwende stieg der
Prozessortakt kontinuierlich an und die Ausführung einzelner Instruktionen
wurde optimiert.
Allerdings stagniert der Trend seitdem.
Es sind physikalische Limits erreicht; ein weiterer Anstieg kann nur noch
mit hohem Energieverbrauch und viel Abwärme erkauft werden.
Der Nutzen wird einschränkt.
Die Aussagen bedeuten jedoch nicht, dass moderne Prozessoren nicht effizienter
sein können.
Die Multicore-Architekturen setzen mehrere Prozessoren auf einem Chip ein,
zudem wird Hyper-Threading standardmäßig genutzt.
Aber ein Programm läuft nicht von allein viermal so schnell, wenn man es auf
einer Maschine mit Dualcore Prozessor einsetzt.
Im Gegensatz zur Erhöhung der Taktfrequenz, kann Software nicht einfach mehrere
Kerne verwenden.
Wie Herb Sutter in seinem Artikel „The Free Lunch Is Over“
\cite{sutterlunch} beschreibt, ist es heute notwendig, die Programme anzupassen
und es auszunutzen, dass nebenläufige Ausführung möglich ist.
Man kann nicht mehr darauf vertrauen, dass die Software mit jeder Generation
von Prozessoren schneller läuft.
Die Algorithmen müssen angepasst und parallelisiert werden, falls möglich, um
eine effiziente Auslastung von modernen Computer zu gewährleisten.

Diese Ausarbeitung soll einen groben Überblick über parallele Algorithmen geben.
Es werden theoretische Modelle vorgestellt, anhand derer Algorithmen analysiert
werden können, und es wird auf die Frage eingegangen, welche Probleme überhaupt
parallel gelöst werden können.

\subsection{Der Begriff Parallelität}

\subsection{Aufbau}
In den ersten beiden Abschnitten werden abstrakte Modelle vorgestellt, anhand
derer sequentielle und parallele Algorithmen betrachtet werden können.
Kapitel 4 behandelt die Komplexitätsklassen der beobachteten Algorithmen.
Es wird darauf eingegangen, welche Probleme sich überhaupt effizient parallel
gelöst werden können.

Im Laufe des Textes werden verschiedene Angaben für Komplexität verwendet.
Dabei wird ein uniformes Komplexitätsmaß verwendet.
Die Menge an verwendeten Variablen, in denen Werte während der Ausführung
zwischengespeichert werden, ist die Platzkomplexität ($space$).
Die Zeitkomplexität ($time$) eines Algorithmus gibt die Anzahl der
Zeiteinheiten an, die zur Ausführung gebraucht werden.
Eine Anweisung benötigt eine Zeiteinheit.
Allerdings können insbesondere bei parallelen Algorithmen Anweisungen parallel
ausgeführt werden.
Die Zeitkomplexität entspricht daher nicht unbedingt der Anzahl ausgeführter
Anweisungen ($work$).
Diese wird auch als Operationenkomplexität bezeichnet.
Des weiteren wird bei parallelen Algorithmen die Anzahl der verwendeten
Prozessoren betrachtet.
